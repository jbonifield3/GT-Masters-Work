{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b><center> Homework 4 </center></b>\n",
    "<br>\n",
    "<b><center> James Bonifield </center></b>\n",
    "<br>\n",
    "<br>\n",
    "<b>Question 7.1</b><br>\n",
    "Describe a situation or problem from your job, everyday life, current events, etc., for which exponential\n",
    "smoothing would be appropriate. What data would you need? Would you expect the value of $\\alpha$ (the\n",
    "first smoothing parameter) to be closer to 0 or 1, and why? <br> <br>\n",
    "<i> In my current role as a technical consultant for a software company, I often need to review usage logs and draw conclusions from noisy performance data taken at the minute or even second level of detail where a smoothing model would be very applicable to reduce said noise. I would likely chose a large to mid sized $\\alpha$ value, since I want to be able to detect peaks without too much smoothing that a low $\\alpha$ value might overwrite.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 7.2</b><br>\n",
    "Using the 20 years of daily high temperature data for Atlanta (July through October) from Question 6.2,\n",
    "build and use an exponential smoothing model to help make a judgment of whether the unofficial end\n",
    "of summer has gotten later over the 20 years. (Part of the point of this assignment is for you to think\n",
    "about how you might use exponential smoothing to answer this question. Feel free to combine it with\n",
    "other models if you’d like to. There’s certainly more than one reasonable approach.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-- Attaching packages --------------------------------------- tidyverse 1.2.1 --\n",
      "<U+221A> ggplot2 2.2.1     <U+221A> purrr   0.2.4\n",
      "<U+221A> tibble  1.4.1     <U+221A> dplyr   0.7.4\n",
      "<U+221A> tidyr   0.7.2     <U+221A> stringr 1.2.0\n",
      "<U+221A> readr   1.1.1     <U+221A> forcats 0.2.0\n",
      "-- Conflicts ------------------------------------------ tidyverse_conflicts() --\n",
      "x dplyr::filter() masks stats::filter()\n",
      "x dplyr::lag()    masks stats::lag()\n",
      "\n",
      "Attaching package: 'lubridate'\n",
      "\n",
      "The following object is masked from 'package:base':\n",
      "\n",
      "    date\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# A tibble: 123 x 21\n",
      "   DAY    `1996` `1997` `1998` `1999` `2000` `2001` `2002` `2003` `2004` `2005`\n",
      "   <chr>   <int>  <int>  <int>  <int>  <int>  <int>  <int>  <int>  <int>  <int>\n",
      " 1 1-Jul      98     86     91     84     89     84     90     73     82     91\n",
      " 2 2-Jul      97     90     88     82     91     87     90     81     81     89\n",
      " 3 3-Jul      97     93     91     87     93     87     87     87     86     86\n",
      " 4 4-Jul      90     91     91     88     95     84     89     86     88     86\n",
      " 5 5-Jul      89     84     91     90     96     86     93     80     90     89\n",
      " 6 6-Jul      93     84     89     91     96     87     93     84     90     82\n",
      " 7 7-Jul      93     75     93     82     96     87     89     87     89     76\n",
      " 8 8-Jul      91     87     95     86     91     89     89     90     87     88\n",
      " 9 9-Jul      93     84     95     87     96     91     90     89     88     89\n",
      "10 10-Jul     93     87     91     87     99     87     91     84     89     78\n",
      "# ... with 113 more rows, and 10 more variables: `2006` <int>, `2007` <int>,\n",
      "#   `2008` <int>, `2009` <int>, `2010` <int>, `2011` <int>, `2012` <int>,\n",
      "#   `2013` <int>, `2014` <int>, `2015` <int>\n"
     ]
    }
   ],
   "source": [
    "library(tidyverse)\n",
    "library(lubridate)\n",
    "TempData <- suppressMessages(read_tsv(\"temps.txt\"))\n",
    "print(TempData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Below is my attempt to 'tidyverse' data into a ts. It didn't really work that well, so you can just\n",
    "#Skip down to the tsData info\n",
    "FlatTempData <- as_tibble(gather(TempData, '1996','1997','1998','1999','2000','2001',\n",
    "                    '2002','2003','2004','2005','2006','2007','2008','2009',\n",
    "                    '2010','2011','2012','2013','2014','2015', key = 'Year', value = 'Temps'))\n",
    "FlatTempData <- mutate(FlatTempData, DAY= dmy(paste(DAY,Year,sep = '-')))\n",
    "FlatTempData$Year <- NULL\n",
    "FlatTempData$DAY <- NULL\n",
    "\n",
    "#Convert to Time Series\n",
    "tsData <- ts(FlatTempData,start=1996, frequency=123)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's take a look at the Double Exponential Smoothing Model (no seasonality) to get an idea of the trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Holt-Winters exponential smoothing with trend and without seasonal component.\n",
       "\n",
       "Call:\n",
       "HoltWinters(x = tsData, gamma = FALSE, seasonal = \"multiplicative\")\n",
       "\n",
       "Smoothing parameters:\n",
       " alpha: 0.8445729\n",
       " beta : 0.003720884\n",
       " gamma: FALSE\n",
       "\n",
       "Coefficients:\n",
       "        [,1]\n",
       "a 63.2530022\n",
       "b -0.0729933"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Generate Model\n",
    "?HoltWinters\n",
    "hwDoubleExpModel <- HoltWinters(tsData, gamma=FALSE)\n",
    "hwDoubleExpModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these results we see that the trend (b) is almost zero and can pretty much be ignored in our analysis for seasonality (I think, not 100% on this one - let me know in the comments) <br>\n",
    "Now we take a look at the data with seasonality, and store the seasonality in a 123x19 matrix (days x years) to run CUSUM on each day of the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Holt-Winters exponential smoothing with trend and multiplicative seasonal component.\n",
       "\n",
       "Call:\n",
       "HoltWinters(x = tsData, seasonal = \"multiplicative\")\n",
       "\n",
       "Smoothing parameters:\n",
       " alpha: 0.615003\n",
       " beta : 0\n",
       " gamma: 0.5495256\n",
       "\n",
       "Coefficients:\n",
       "             [,1]\n",
       "a    73.679517064\n",
       "b    -0.004362918\n",
       "s1    1.239022317\n",
       "s2    1.234344062\n",
       "s3    1.159509551\n",
       "s4    1.175247483\n",
       "s5    1.171344196\n",
       "s6    1.151038408\n",
       "s7    1.139383104\n",
       "s8    1.130484528\n",
       "s9    1.110487514\n",
       "s10   1.076242879\n",
       "s11   1.041044609\n",
       "s12   1.058139281\n",
       "s13   1.032496529\n",
       "s14   1.036257448\n",
       "s15   1.019348815\n",
       "s16   1.026754142\n",
       "s17   1.071170378\n",
       "s18   1.054819556\n",
       "s19   1.084397734\n",
       "s20   1.064605879\n",
       "s21   1.109827336\n",
       "s22   1.112670130\n",
       "s23   1.103970506\n",
       "s24   1.102771209\n",
       "s25   1.091264692\n",
       "s26   1.084518342\n",
       "s27   1.077914660\n",
       "s28   1.077696145\n",
       "s29   1.053788854\n",
       "s30   1.079454300\n",
       "s31   1.053481186\n",
       "s32   1.054023885\n",
       "s33   1.078221405\n",
       "s34   1.070145761\n",
       "s35   1.054891375\n",
       "s36   1.044587771\n",
       "s37   1.023285461\n",
       "s38   1.025836722\n",
       "s39   1.031075732\n",
       "s40   1.031419152\n",
       "s41   1.021827552\n",
       "s42   0.998177248\n",
       "s43   0.996049257\n",
       "s44   0.981570825\n",
       "s45   0.976510542\n",
       "s46   0.967977608\n",
       "s47   0.985788411\n",
       "s48   1.004748195\n",
       "s49   1.050965934\n",
       "s50   1.072515008\n",
       "s51   1.086532279\n",
       "s52   1.098357400\n",
       "s53   1.097158461\n",
       "s54   1.054827180\n",
       "s55   1.022866587\n",
       "s56   0.987259326\n",
       "s57   1.016923524\n",
       "s58   1.016604903\n",
       "s59   1.004320951\n",
       "s60   1.019102781\n",
       "s61   0.983848662\n",
       "s62   1.055888360\n",
       "s63   1.056122844\n",
       "s64   1.043478958\n",
       "s65   1.039475693\n",
       "s66   0.991019224\n",
       "s67   1.001437488\n",
       "s68   1.002221759\n",
       "s69   1.003949213\n",
       "s70   0.999566344\n",
       "s71   1.018636837\n",
       "s72   1.026490773\n",
       "s73   1.042507768\n",
       "s74   1.022500795\n",
       "s75   1.002503740\n",
       "s76   1.004560984\n",
       "s77   1.025536556\n",
       "s78   1.015357769\n",
       "s79   0.992176558\n",
       "s80   0.979377825\n",
       "s81   0.998058079\n",
       "s82   1.002553395\n",
       "s83   0.955429116\n",
       "s84   0.970970220\n",
       "s85   0.975543504\n",
       "s86   0.931515830\n",
       "s87   0.926764603\n",
       "s88   0.958565273\n",
       "s89   0.963250387\n",
       "s90   0.951644060\n",
       "s91   0.937362688\n",
       "s92   0.954257999\n",
       "s93   0.892485444\n",
       "s94   0.879537700\n",
       "s95   0.879946892\n",
       "s96   0.890633648\n",
       "s97   0.917134959\n",
       "s98   0.925991769\n",
       "s99   0.884247686\n",
       "s100  0.846648167\n",
       "s101  0.833696369\n",
       "s102  0.800001437\n",
       "s103  0.807934782\n",
       "s104  0.819343668\n",
       "s105  0.828571029\n",
       "s106  0.795608740\n",
       "s107  0.796609993\n",
       "s108  0.815503509\n",
       "s109  0.830111282\n",
       "s110  0.829086181\n",
       "s111  0.818367239\n",
       "s112  0.863958784\n",
       "s113  0.912057203\n",
       "s114  0.898308248\n",
       "s115  0.878723779\n",
       "s116  0.848971946\n",
       "s117  0.813891909\n",
       "s118  0.846821392\n",
       "s119  0.819121827\n",
       "s120  0.851036184\n",
       "s121  0.820416491\n",
       "s122  0.851581233\n",
       "s123  0.874038407"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hwSeasonalNoTrend <-HoltWinters(tsData, seasonal='multiplicative')\n",
    "hwSeasonalNoTrend\n",
    "SeasonalData <- as.tibble(matrix(hwSeasonalNoTrend$fitted[,4],ncol=19))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing the data to CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_csv(SeasonalData,path='Seasonal_Data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysing the data in Excel with two CUSUM analysis (one for each tail),I found no discernable difference between the early and later year cohorts of seasonality, and conclude like in the previous assignment that this data does not suggest one way or another about changing summer dates or temperatures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "r"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
